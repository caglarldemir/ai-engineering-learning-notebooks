{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents Using LangChain for Different Sources \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **20** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are working as a data scientist at a consulting firm, and you've been tasked with analyzing documents from multiple clients. Each client provides their data in different formats: some in PDFs, others in Word documents, CSV files, or even HTML webpages. Manually loading and parsing each document type is not only time-consuming but also prone to errors. Your goal is to streamline this process, making it efficient and error-free.\n",
    "\n",
    "To achieve this, you'll use LangChain’s powerful document loaders. These loaders allow you to read and convert various file formats into a unified document structure that can be easily processed. For example, you'll load client policy documents from text files, financial reports from PDFs, marketing strategies from Word documents, and product reviews from JSON files. By the end of this lab, you will have a robust pipeline that can handle any new file formats clients might send, saving you valuable time and effort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Hvf-jk8b5Fs-E_E4AJyEow/loader.png\" width=\"50%\" alt=\"indexing\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will explore how to use various loaders provided by LangChain to load and process data from different file formats. These loaders simplify the task of reading and converting files into a document format that can be processed downstream. By the end of this lab, you will be able to efficiently load text, PDF, Markdown, JSON, CSV, DOCX, and other file formats into a unified format, allowing for seamless data analysis and manipulation for LLM applications.\n",
    "\n",
    "(Note: In this lab, we just introduced several commonly used file format loaders. LangChain provides more document loaders for various document formats [here](https://python.langchain.com/v0.2/docs/integrations/document_loaders/).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Load-from-TXT-files\">Load from TXT files</a></li>\n",
    "    <li><a href=\"#Load-from-PDF-files\">Load from PDF files</a></li>\n",
    "    <li><a href=\"#Load-from-Markdown-files\">Load from Markdown files</a></li>\n",
    "    <li><a href=\"#Load-from-JSON-files\">Load from JSON files</a></li>\n",
    "    <li><a href=\"#Load-from-CSV-files\">Load from CSV files</a></li>\n",
    "    <li><a href=\"#Load-from-URL/Website-files\">Load from URL/Webpage files</a></li>\n",
    "    <li><a href=\"#Load-from-WORD-files\">Load from WORD files</a></li>\n",
    "    <li><a href=\"#Load-from-Unstructured-Files\">Load from Unstructured Files</a></li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"#Exercises\">Exercises</a>\n",
    "<ol>\n",
    "    <li><a href=\"#Exercise-1---Try-to-use-other-PDF-loaders\">Exercise 1 - Try to use other PDF loaders</a></li>\n",
    "    <li><a href=\"#Exercise-2---Load-from-Arxiv\">Exercise 2 - Load from Arxiv</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Understand how to use `TextLoader` to load text files.\n",
    " - Learn how to load PDFs using `PyPDFLoader` and `PyMuPDFLoader`.\n",
    " - Use `UnstructuredMarkdownLoader` to load Markdown files.\n",
    " - Load JSON files with `JSONLoader` using jq schemas.\n",
    " - Process CSV files with `CSVLoader` and `UnstructuredCSVLoader`.\n",
    " - Load Webpage content using `WebBaseLoader`.\n",
    " - Load Word documents using `Docx2txtLoader`.\n",
    " - Utilize `UnstructuredFileLoader` for various file types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are __not__ preinstalled in the Skills Network Labs environment. __You must run the following cell__ to install them:\n",
    "\n",
    "**Note:** We are pinning the version here to specify the version. It's recommended that you do this as well. Even if the library is updated in the future, the installed library could still support this lab work.\n",
    "\n",
    "This might take approximately 1 minute. \n",
    "\n",
    "As we use `%%capture` to capture the installation, you won't see the output process. But after the installation completes, you will see a number beside the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "#After executing the cell,please RESTART the kernel and run all the cells.\n",
    "!pip install --user \"langchain-community==0.2.1\"\n",
    "!pip install --user \"pypdf==4.2.0\"\n",
    "!pip install --user \"PyMuPDF==1.24.5\"\n",
    "!pip install --user \"unstructured==0.14.8\"\n",
    "!pip install --user \"markdown==3.6\"\n",
    "!pip install --user  \"jq==1.7.0\"\n",
    "!pip install --user \"pandas==2.2.2\"\n",
    "!pip install --user \"docx2txt==0.8\"\n",
    "!pip install --user \"requests==2.32.3\"\n",
    "!pip install --user \"beautifulsoup4==4.12.3\"\n",
    "!pip install --user \"nltk==3.8.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you install the libraries, restart your kernel. You can do that by clicking the **Restart the kernel** icon.\n",
    "\n",
    "<p style=\"text-align:left\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/1_Bd_EvpEzLH9BbxRXXUGQ/screenshot-to-replace.png\" width=\"50%\"/>\n",
    "    </a>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders.csv_loader import UnstructuredCSVLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from TXT files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TextLoader` is a tool designed to load textual data from various sources.\n",
    "\n",
    "It is the simplest loader, reading a file as text and placing all the content into a single document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a .txt file for you to load. First, we need to download it from a remote source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a .txt file for you to load. First, we need to download it from a remote source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-04 13:45:22--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 6363 (6.2K) [text/plain]\n",
      "Saving to: ‘new-Policies.txt’\n",
      "\n",
      "new-Policies.txt    100%[===================>]   6.21K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-08-04 13:45:22 (620 MB/s) - ‘new-Policies.txt’ saved [6363/6363]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the `TextLoader` class to load the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x73095e8e8560>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"new-Policies.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the `load` method to load the data as documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's present the entire data (document) here.\n",
    "\n",
    "This is a `document` object that includes `page_content` and `metadata` attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content=\"1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual's contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\\n\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `pprint` function to print the first 1000 characters of the `page_content` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1. Code of Conduct\\n'\n",
      " '\\n'\n",
      " 'Our Code of Conduct establishes the core values and ethical standards that '\n",
      " 'all members of our organization must adhere to. We are committed to '\n",
      " 'fostering a workplace characterized by integrity, respect, and '\n",
      " 'accountability.\\n'\n",
      " '\\n'\n",
      " 'Integrity: We commit to the highest ethical standards by being honest and '\n",
      " 'transparent in all our dealings, whether with colleagues, clients, or the '\n",
      " 'community. We protect sensitive information and avoid conflicts of '\n",
      " 'interest.\\n'\n",
      " '\\n'\n",
      " \"Respect: We value diversity and every individual's contribution. \"\n",
      " 'Discrimination, harassment, or any form of disrespect is not tolerated. We '\n",
      " 'promote an inclusive environment where differences are respected, and '\n",
      " 'everyone is treated with dignity.\\n'\n",
      " '\\n'\n",
      " 'Accountability: We are responsible for our actions and decisions, complying '\n",
      " 'with all relevant laws and regulations. We aim for continuous improvement '\n",
      " 'and report any breaches of this code, supporting investigations into such '\n",
      " 'matters.\\n'\n",
      " '\\n'\n",
      " 'Safety: We prioritize the safety of our employees, c')\n"
     ]
    }
   ],
   "source": [
    "pprint(data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from PDF files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we may have files in PDF format that we want to load for processing.\n",
    "\n",
    "LangChain provides several classes for loading PDFs. Here, we introduce two classes: `PyPDFLoader` and `PyMuPDFLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the PDF using `PyPDFLoader` into an array of documents, where each document contains the page content and metadata with the page number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first page of the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1 I NTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article }) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first three pages of the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p,page in enumerate(pages[0:3]):\n",
    "    print(f\"page number {p+1}\")\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyMuPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyMuPDFLoader` is the fastest of the PDF parsing options. It provides detailed metadata about the PDF and its pages, and returns one document per page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(pdf_url)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `metadata` attribute reveals that `PyMuPDFLoader` provides more detailed metadata information than `PyPDFLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Markdown files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, our file source might be in Markdown format.\n",
    "\n",
    "LangChain provides the `UnstructuredMarkdownLoader` to load content from Markdown files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_path = \"markdown-sample.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from JSON files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSONLoader uses a specified [jq schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files. It uses the jq python package, which we've installed before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use `pprint` to take a look at the JSON file and its structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='facebook-chat.json'\n",
    "data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `JSONLoader` to load data from the JSON file. However, JSON files can have various attribute-value pairs. If we want to load a specific attribute and its value, we need to set an appropriate `jq schema`.\n",
    "\n",
    "So for example, if we want to load the `content` from the JSON file, we need to set `jq_schema='.messages[].content'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from CSV files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV files are a common format for storing tabular data. The `CSVLoader` provides a convenient way to read and process this data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='mlb-teams-2012.csv')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you load data from a CSV file, the loader typically creates a separate `Document` object for each row of data in the CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UnstructuredCSVLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to `CSVLoader`, which treats each row as an individual document with headers defining the data, `UnstructuredCSVLoader` considers the entire CSV file as a single unstructured table element. This approach is beneficial when you want to analyze the data as a complete table rather than as separate entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredCSVLoader(\n",
    "    file_path=\"mlb-teams-2012.csv\", mode=\"elements\"\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].metadata[\"text_as_html\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from URL/Website files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we use `BeautifulSoup` package to load and parse a HTML or XML file. But it has some limitations.\n",
    "\n",
    "The following code is using `BeautifulSoup` to parse a website. Let's see what limitation it has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.ibm.com/topics/langchain'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the print output, we can see that `BeautifulSoup` not only load the web content, but also a lot of HTML tags and external links, which are not necessary if we just want to load the text content of the web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So LangChain's `WebBaseLoader` can effectively address this limitation.\n",
    "\n",
    "`WebBaseLoader` is designed to extract all text from HTML webpages and convert it into a document format suitable for further processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from single web page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://www.ibm.com/topics/langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from multiple web pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load multiple webpages simultaneously by passing a list of URLs to the loader. This will return a list of documents corresponding to the order of the URLs provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader([\"https://www.ibm.com/topics/langchain\", \"https://www.redhat.com/en/topics/ai/what-is-instructlab\"])\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from WORD files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Docx2txtLoader` is utilized to convert Word documents into a document format suitable for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/94hiHUNLZdb0bLMkrCh79g/file-sample.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Docx2txtLoader(\"file-sample.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Unstructured Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we need to load content from various text sources and formats without writing a separate loader for each one. Additionally, when a new file format emerges, we want to save time by not having to write a new loader for it. `UnstructuredFileLoader` addresses this need by supporting the loading of multiple file types. Currently, `UnstructuredFileLoader` can handle text files, PowerPoints, HTML, PDFs, images, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can load `.txt` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"new-Policies.txt\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can load `.md` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"markdown-sample.md\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple files with different formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even load a list of files with different formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"markdown-sample.md\", \"new-Policies.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Try to use other PDF loaders\n",
    "\n",
    "There are many other PDF loaders in LangChain, for example, `PyPDFium2Loader`. Can you use this PDF loader to load the PDF and see the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "!pip install pypdfium2\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "loader = PyPDFium2Loader(pdf_url)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Load from Arxiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we have paper that we want to load from Arxiv, can you load this [paper](https://arxiv.org/abs/1605.08386) using `ArxivLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "!pip install arxiv\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1605.08386\", load_max_docs=2).load()\n",
    "\n",
    "print(docs[0].page_content[:1000])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kang Wang](https://www.linkedin.com/in/kangwang95/)\n",
    "\n",
    "Kang Wang is a Data Scientist in IBM. He is also a PhD Candidate in the University of Waterloo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/)\n",
    "\n",
    "Joseph has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Hailey Quach](https://author.skills.network/instructors/hailey_quach)\n",
    "\n",
    "Hailey is a Data Scientist at IBM. She is also an undergraduate student at Concordia University, Montreal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "8ccd87848ab9e79d16c766e68c2292b6bf1eff17098bb52f22c15a7b9da59990"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
